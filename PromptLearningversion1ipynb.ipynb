{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanuhya/design-project/blob/main/PromptLearningversion1ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FHw9iRI6gx0",
        "outputId": "791a8180-fae5-4d6b-cfcb-e02a9cbd37e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LPFeBnGtrHpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yodiKj486v7R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Transfer-Learning-System-for-Text-Classification-main/SRC\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii0MD4cVJVfG",
        "outputId": "32e0b670-b246-4ae5-db67-a6932d6d2a32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (69.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U setuptools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZy4y8GJL7WE",
        "outputId": "d324913b-cece-428f-9662-35262f34c2a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "32 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "/bin/bash: line 1: !apt: command not found\n"
          ]
        }
      ],
      "source": [
        "!apt update && !apt upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PydqixnVORUr",
        "outputId": "f27bbdca-d1e4-40a2-8c19-261138d6ee36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ7JLyTqOm80"
      },
      "outputs": [],
      "source": [
        "# transformers==4.6.0\n",
        "# accelerate==0.3.0\n",
        "# pyarrow==4.0.1\n",
        "# datasets==1.6.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMdWdjxdwqxy",
        "outputId": "a7f5eb60-2890-41eb-9ceb-f3b1710e5e7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "transformers\n",
        "accelerate\n",
        "pyarrow\n",
        "datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcX6Ae7-xMQG",
        "outputId": "37c06116-c90c-46ce-9120-bed62ac96fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.35.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.24.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (9.0.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 3)) (2.1.0+cu118)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 5)) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 5)) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 5)) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 5)) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 5)) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 5)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 5)) (3.8.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers->-r requirements.txt (line 2)) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 3)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 3)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 3)) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 5)) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate->-r requirements.txt (line 3)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate->-r requirements.txt (line 3)) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3puDJYno5-xa"
      },
      "outputs": [],
      "source": [
        "task_input_key = {\n",
        "    \"sst2\": [\"sentence\"],\n",
        "    \"sst5\": [\"sentence\"],\n",
        "    \"cola\": [\"sentence\"],\n",
        "    \"mnli\": [\"sentence1\", \"sentence2\"],\n",
        "    \"mrpc\": [\"#1 String\", \"#2 String\"],\n",
        "    \"qnli\": [\"question\", \"sentence\"],\n",
        "    \"qqp\": [\"question1\", \"question2\"],\n",
        "    \"rte\": [\"sentence1\", \"sentence2\"],\n",
        "}\n",
        "\n",
        "task_label_key = {\n",
        "    \"sst2\": \"label\",\n",
        "    \"sst5\": \"label\",\n",
        "    \"cola\": \"label\",\n",
        "    \"mnli\": \"gold_label\",\n",
        "    \"mrpc\": \"Quality\",\n",
        "    \"qnli\": \"label\",\n",
        "    \"qqp\": \"is_duplicate\",\n",
        "    \"rte\": \"label\",\n",
        "}\n",
        "\n",
        "task_metric = {\n",
        "    \"sst2\": \"accuracy\",\n",
        "    \"sst5\": \"accuracy\",\n",
        "    \"cola\": \"matthews_correlation\",\n",
        "    \"mnli\": \"accuracy\",\n",
        "    \"qnli\": \"accuracy\",\n",
        "    \"rte\": \"accuracy\",\n",
        "    \"mrpc\": \"f1\",\n",
        "    \"qqp\": \"f1\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jqli9unMxgyi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertForSequenceClassification, BertModel, BertOnlyMLMHead\n",
        "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel, RobertaModel, RobertaLMHead, RobertaClassificationHead\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class RobertaForPromptFinetuning(RobertaPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.lm_head = RobertaLMHead(config)\n",
        "        self.init_weights()\n",
        "\n",
        "        self.label_token_list = None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        mask_pos=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        if mask_pos is not None:\n",
        "            mask_pos = mask_pos.squeeze()\n",
        "\n",
        "        # Encode everything\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # Get <mask> token representation\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        sequence_mask_output = sequence_output[torch.arange(sequence_output.size(0)), mask_pos]\n",
        "\n",
        "        # Logits over vocabulary tokens\n",
        "        prediction_mask_scores = self.lm_head(sequence_mask_output)\n",
        "#         all_logits = prediction_mask_scores\n",
        "        all_logits = F.softmax(prediction_mask_scores, dim=-1)\n",
        "\n",
        "        if self.label_token_list is not None:\n",
        "            logits = []\n",
        "            for label in self.label_token_list:\n",
        "                logits.append(torch.sum(all_logits[:, self.label_token_list[label]], 1).unsqueeze(-1))\n",
        "            logits = torch.cat(logits, -1)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "#             loss_fct = nn.CrossEntropyLoss()\n",
        "            loss_fct = nn.NLLLoss()\n",
        "#             loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "            loss = loss_fct(torch.log(logits.view(-1, logits.size(-1))), labels.view(-1))\n",
        "\n",
        "        output = (all_logits,)\n",
        "        if self.label_token_list is not None:\n",
        "            output = ((logits,) + output)\n",
        "        return ((loss,) + output) if loss is not None else output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXOytxZSxuoy"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import json\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset, load_metric\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import transformers\n",
        "from accelerate import Accelerator\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    PretrainedConfig,\n",
        "    SchedulerType,\n",
        "    default_data_collator,\n",
        "    get_scheduler,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using readlines()\n",
        "file1 = open('/content/drive/MyDrive/Transfer-Learning-System-for-Text-Classification-main/SRC/data/auto_template/SST-2/16-42.txt', 'r')\n",
        "Lines = file1.readlines()\n",
        "\n",
        "count = 0\n",
        "# Strips the newline character\n",
        "for line in Lines:\n",
        "    count += 1\n",
        "    print(\"Line{}: {}\".format(count, line.strip()))"
      ],
      "metadata": {
        "id": "_2l9ujCXSqMM",
        "outputId": "f0a28c86-f4ad-480a-c097-c09d1f8faa1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Line1: *cls**sent_0*_A*mask*_movie.*sep+*\n",
            "Line2: *cls**sent_0*_A*mask*_film.*sep+*\n",
            "Line3: *cls**sent_0*_This_is*mask*.*sep+*\n",
            "Line4: *cls**sent_0*_It's*mask*.*sep+*\n",
            "Line5: *cls**sent_0*_It_is*mask*.*sep+*\n",
            "Line6: *cls**sent_0*_It_was*mask*.*sep+*\n",
            "Line7: *cls**sent_0*_A*mask*_experience.*sep+*\n",
            "Line8: *cls**sent_0*_This_is*mask*!*sep+*\n",
            "Line9: *cls**sent_0*_It’s*mask*.*sep+*\n",
            "Line10: *cls**sent_0*_Its*mask*.*sep+*\n",
            "Line11: *cls**sent_0*_A*mask*_watch.*sep+*\n",
            "Line12: *cls**sent_0*_A*mask*_read.*sep+*\n",
            "Line13: *cls**sent_0*_A_truly*mask*_film.*sep+*\n",
            "Line14: *cls**sent_0*_It's*mask*!*sep+*\n",
            "Line15: *cls**sent_0*_A*mask*_story.*sep+*\n",
            "Line16: *cls**sent_0*_It_is*mask*!*sep+*\n",
            "Line17: *cls**sent_0*_A_truly*mask*_movie.*sep+*\n",
            "Line18: *cls**sent_0*_A*mask*_show.*sep+*\n",
            "Line19: *cls**sent_0*_It_was*mask*!*sep+*\n",
            "Line20: *cls**sent_0*_A*mask*_movie!*sep+*\n",
            "Line21: *cls**sent_0*_A*mask*_time.*sep+*\n",
            "Line22: *cls**sent_0*_A*mask*_one.*sep+*\n",
            "Line23: *cls**sent_0*_This_was*mask*.*sep+*\n",
            "Line24: *cls**sent_0*_Looks*mask*.*sep+*\n",
            "Line25: *cls**sent_0*_A*mask*_piece.*sep+*\n",
            "Line26: *cls**sent_0*_A*mask*_book.*sep+*\n",
            "Line27: *cls**sent_0*_Truly*mask*.*sep+*\n",
            "Line28: *cls**sent_0*_A*mask*_start.*sep+*\n",
            "Line29: *cls**sent_0*_A*mask*_film!*sep+*\n",
            "Line30: *cls**sent_0*_Just*mask*.*sep+*\n",
            "Line31: *cls**sent_0*_A*mask*_idea.*sep+*\n",
            "Line32: *cls**sent_0*_The_music_is*mask*.*sep+*\n",
            "Line33: *cls**sent_0*_it's*mask*.*sep+*\n",
            "Line34: *cls**sent_0*_A_really*mask*_film.*sep+*\n",
            "Line35: *cls**sent_0*_A*mask*_choice.*sep+*\n",
            "Line36: *cls**sent_0*_A*mask*_performance.*sep+*\n",
            "Line37: *cls**sent_0*_A_really*mask*_movie.*sep+*\n",
            "Line38: *cls**sent_0*_I_thought_it_was*mask*.*sep+*\n",
            "Line39: *cls**sent_0*_All_in_all*mask*.*sep+*\n",
            "Line40: *cls**sent_0*_A*mask*_waste_of_time.*sep+*\n",
            "Line41: *cls**sent_0*_That_is*mask*.*sep+*\n",
            "Line42: *cls**sent_0*_A*mask*_work.*sep+*\n",
            "Line43: *cls**sent_0*_A*mask*_disappointment.*sep+*\n",
            "Line44: *cls**sent_0*_A*mask*_job.*sep+*\n",
            "Line45: *cls**sent_0*_A_truly*mask*_experience.*sep+*\n",
            "Line46: *cls**sent_0*_it_was*mask*.*sep+*\n",
            "Line47: *cls**sent_0*_A*mask*_effort.*sep+*\n",
            "Line48: *cls**sent_0*_It’s*mask*!*sep+*\n",
            "Line49: *cls**sent_0*_A*mask*_day.*sep+*\n",
            "Line50: *cls**sent_0*_it_is*mask*.*sep+*\n",
            "Line51: *cls*_This_is*mask*.*+sent_0**sep+*\n",
            "Line52: *cls*_It's*mask*.*+sent_0**sep+*\n",
            "Line53: *cls*_It_was*mask*.*+sent_0**sep+*\n",
            "Line54: *cls*_A*mask*_movie.*+sent_0**sep+*\n",
            "Line55: *cls*_It_is*mask*.*+sent_0**sep+*\n",
            "Line56: *cls*_A*mask*_film.*+sent_0**sep+*\n",
            "Line57: *cls*_It’s*mask*.*+sent_0**sep+*\n",
            "Line58: *cls*_Just*mask*.*+sent_0**sep+*\n",
            "Line59: *cls*_This_is*mask*!*+sent_0**sep+*\n",
            "Line60: *cls*_It's*mask*!*+sent_0**sep+*\n",
            "Line61: *cls*_This_was*mask*.*+sent_0**sep+*\n",
            "Line62: *cls*_Its*mask*.*+sent_0**sep+*\n",
            "Line63: *cls*_A*mask*_experience.*+sent_0**sep+*\n",
            "Line64: *cls*_It_was*mask*!*+sent_0**sep+*\n",
            "Line65: *cls*_It_is*mask*!*+sent_0**sep+*\n",
            "Line66: *cls*_A*mask*_story.*+sent_0**sep+*\n",
            "Line67: *cls*_Really*mask*.*+sent_0**sep+*\n",
            "Line68: *cls*_I_thought_it_was*mask*.*+sent_0**sep+*\n",
            "Line69: *cls*_So*mask*.*+sent_0**sep+*\n",
            "Line70: *cls*_Absolutely*mask*.*+sent_0**sep+*\n",
            "Line71: *cls*_It’s*mask*!*+sent_0**sep+*\n",
            "Line72: *cls*_That's*mask*.*+sent_0**sep+*\n",
            "Line73: *cls*_A*mask*_show.*+sent_0**sep+*\n",
            "Line74: *cls*_A*mask*_watch.*+sent_0**sep+*\n",
            "Line75: *cls*_The_music_is*mask*.*+sent_0**sep+*\n",
            "Line76: *cls*_The_film_is*mask*.*+sent_0**sep+*\n",
            "Line77: *cls*_That_was*mask*.*+sent_0**sep+*\n",
            "Line78: *cls*_That_is*mask*.*+sent_0**sep+*\n",
            "Line79: *cls*_Very*mask*.*+sent_0**sep+*\n",
            "Line80: *cls*_A*mask*_one.*+sent_0**sep+*\n",
            "Line81: *cls*_A*mask*_read.*+sent_0**sep+*\n",
            "Line82: *cls*_This_movie_is*mask*.*+sent_0**sep+*\n",
            "Line83: *cls*_A*mask*_piece_of_work.*+sent_0**sep+*\n",
            "Line84: *cls*_It_looks*mask*.*+sent_0**sep+*\n",
            "Line85: *cls*_The_story_is*mask*.*+sent_0**sep+*\n",
            "Line86: *cls*_Looks*mask*.*+sent_0**sep+*\n",
            "Line87: *cls*_Just*mask*!*+sent_0**sep+*\n",
            "Line88: *cls*_Not*mask*.*+sent_0**sep+*\n",
            "Line89: *cls*_The_movie_is*mask*.*+sent_0**sep+*\n",
            "Line90: *cls*_Simply*mask*.*+sent_0**sep+*\n",
            "Line91: *cls*_A*mask*_movie!*+sent_0**sep+*\n",
            "Line92: *cls*_A*mask*_book.*+sent_0**sep+*\n",
            "Line93: *cls*_A*mask*_idea.*+sent_0**sep+*\n",
            "Line94: *cls*_This_is_just*mask*.*+sent_0**sep+*\n",
            "Line95: *cls*_This_was*mask*!*+sent_0**sep+*\n",
            "Line96: *cls*_This_is*mask*_stuff.*+sent_0**sep+*\n",
            "Line97: *cls*_This_film_is*mask*.*+sent_0**sep+*\n",
            "Line98: *cls*_It's_just*mask*.*+sent_0**sep+*\n",
            "Line99: *cls*_This_one_is*mask*.*+sent_0**sep+*\n",
            "Line100: *cls*.*mask*.*+sent_0**sep+*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1R1Hh6lK7IE"
      },
      "outputs": [],
      "source": [
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Finetune a transformers model on a text classification task\")\n",
        "    # parser.add_argument(\n",
        "    #     \"--task_name\",\n",
        "    #     type=str,\n",
        "    #     default=None,\n",
        "    #     help=\"The name of the glue task to train on.\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--data_dir\",\n",
        "    #     type=str,\n",
        "    #     default=None,\n",
        "    #     required=True,\n",
        "    #     help=\"A dictionary containing the training, validation, test data.\"\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--model_name_or_path\",\n",
        "    #     type=str,\n",
        "    #     help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
        "    #     required=True,\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--per_device_train_batch_size\",\n",
        "    #     type=int,\n",
        "    #     default=8,\n",
        "    #     help=\"Batch size (per device) for the training dataloader.\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--per_device_eval_batch_size\",\n",
        "    #     type=int,\n",
        "    #     default=8,\n",
        "    #     help=\"Batch size (per device) for the evaluation dataloader.\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--learning_rate\",\n",
        "    #     type=float,\n",
        "    #     default=5e-5,\n",
        "    #     help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--weight_decay\",\n",
        "    #     type=float,\n",
        "    #     default=0.0,\n",
        "    #     help=\"Weight decay to use.\")\n",
        "    # parser.add_argument(\n",
        "    #     \"--num_train_epochs\",\n",
        "    #     type=int,\n",
        "    #     default=3,\n",
        "    #     help=\"Total number of training epochs to perform.\")\n",
        "    # parser.add_argument(\n",
        "    #     \"--max_train_steps\",\n",
        "    #     type=int,\n",
        "    #     default=None,\n",
        "    #     help=\"Total number of training steps to perform. If provided, overrides num_train_epochs.\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--gradient_accumulation_steps\",\n",
        "    #     type=int,\n",
        "    #     default=1,\n",
        "    #     help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--lr_scheduler_type\",\n",
        "    #     type=SchedulerType,\n",
        "    #     default=\"linear\",\n",
        "    #     help=\"The scheduler type to use.\",\n",
        "    #     choices=[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"],\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--num_warmup_steps\",\n",
        "    #     type=int,\n",
        "    #     default=0,\n",
        "    #     help=\"Number of steps for the warmup in the lr scheduler.\"\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--output_dir\",\n",
        "    #     type=str,\n",
        "    #     default=None,\n",
        "    #     help=\"Where to store the final model.\"\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--seed\",\n",
        "    #     type=int,\n",
        "    #     default=None,\n",
        "    #     help=\"A seed for reproducible training.\"\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--shot_num\",\n",
        "    #     type=int,\n",
        "    #     default=None,\n",
        "    #     required=True,\n",
        "    #     help=\"The number of shots to use for training.\"\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--top_k\",\n",
        "    #     type=int,\n",
        "    #     default=10,\n",
        "    #     help=\"Select top k label token for each class.\"\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--eval_steps\",\n",
        "    #     type=int,\n",
        "    #     default=None,\n",
        "    #     help=\"The number of steps to use for evaluation.\"\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--logging_loss_steps\",\n",
        "    #     type=int,\n",
        "    #     default=10,\n",
        "    #     help=\"The number of steps to use for logging the loss.\"\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--template\",\n",
        "    #     type=str,\n",
        "    #     default=None,\n",
        "    #     help=\"The template to use for the output file.\"\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--dedup\",\n",
        "    #     action=\"store_true\",\n",
        "    #     default=False,\n",
        "    #     help=\"Whether to dedup label tokens.\"\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--random_k_token\",\n",
        "    #     action='store_true',\n",
        "    #     default=False,\n",
        "    #     help=\"Whether to random select k label tokens.\"\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"label_token_mode\",\n",
        "    #     type=str,\n",
        "    #     choices=[\"AMuLaP\", \"AutoL\", \"PETAL\"],\n",
        "    #     default=\"AMuLaP\",\n",
        "    #     help=\"How to get the label token.\"\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--mapping_path\",\n",
        "    #     type=str,\n",
        "    #     default=None,\n",
        "    #     help=\"The path to the label token mapping file.\"\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--max_seq_len\",\n",
        "    #     type=int,\n",
        "    #     default=128,\n",
        "    #     help=\"The maximum sequence length.\"\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--first_sent_limit\",\n",
        "    #     type=int,\n",
        "    #     default=None,\n",
        "    #     help=\"The maximum first sentence length.\"\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--other_sent_limit\",\n",
        "    #     type=int,\n",
        "    #     default=None,\n",
        "    #     help=\"The maximum other sentence length.\"\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--no_finetune\",\n",
        "    #     action=\"store_true\",\n",
        "    #     default=False,\n",
        "    #     help=\"Whether to finetune the model.\"\n",
        "    # )\n",
        "\n",
        "    args = argparse.Namespace(task_name='sst2',\n",
        "                              data_dir='/content/drive/MyDrive/Transfer-Learning-System-for-Text-Classification-main/SRC/data/k-shot/SST-2/16-42',\n",
        "                              model_name_or_path='roberta-large',\n",
        "                              per_device_train_batch_size=8,\n",
        "                              per_device_eval_batch_size=8,\n",
        "                              learning_rate=1e-05,\n",
        "                              weight_decay=0.0,\n",
        "                              num_train_epochs=3,\n",
        "                              max_train_steps=1000,\n",
        "                              gradient_accumulation_steps=1,\n",
        "                              lr_scheduler_type=SchedulerType(\"linear\"),\n",
        "                              num_warmup_steps=0,\n",
        "                              output_dir='outputs',\n",
        "                              seed=42,\n",
        "                              shot_num=16,\n",
        "                              top_k=16,\n",
        "                              eval_steps=100,\n",
        "                              logging_loss_steps=10,\n",
        "                              template='*cls**sent_0*_A*mask*_movie.*sep+*',\n",
        "                              dedup=True,\n",
        "                              random_k_token=False,\n",
        "                              label_token_mode='AMuLaP',\n",
        "                              mapping_path=None,\n",
        "                              max_seq_len=128,\n",
        "                              first_sent_limit=None,\n",
        "                              other_sent_limit=None,\n",
        "                              no_finetune=False)\n",
        "\n",
        "\n",
        "    if args.output_dir is not None:\n",
        "        args.logging_dir = os.path.join(args.output_dir, \"logging\", args.task_name, str(args.shot_num) + \"-\" + str(args.seed))\n",
        "        os.makedirs(args.logging_dir, exist_ok=True)\n",
        "        if not args.no_finetune:\n",
        "            dir_name = \"trainstep{}_warmupstep{}_lr{}_pbs{}\".format(args.max_train_steps, args.num_warmup_steps, args.learning_rate, args.per_device_train_batch_size)\n",
        "            dir_name += \"_topk{}\".format(args.top_k)\n",
        "            dir_name += \"_\" + args.label_token_mode\n",
        "            if args.label_token_mode == \"AMuLaP\":\n",
        "                dir_name += \"_random\" if args.random_k_token else \"\"\n",
        "                dir_name += \"_dedup\" if args.dedup else \"\"\n",
        "            args.output_dir = os.path.join(args.output_dir, args.task_name, str(args.shot_num) + \"-\" + str(args.seed), dir_name)\n",
        "            os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KpoKUzgK7X3",
        "outputId": "a5774011-859c-4221-c3d6-c597c1db90e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Namespace(task_name='sst2', data_dir='/content/drive/MyDrive/Transfer-Learning-System-for-Text-Classification-main/SRC/data/k-shot/SST-2/16-42', model_name_or_path='roberta-large', per_device_train_batch_size=8, per_device_eval_batch_size=8, learning_rate=1e-05, weight_decay=0.0, num_train_epochs=3, max_train_steps=1000, gradient_accumulation_steps=1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='outputs/sst2/16-42/trainstep1000_warmupstep0_lr1e-05_pbs8_topk16_AutoL', seed=42, shot_num=16, top_k=16, eval_steps=100, logging_loss_steps=10, template='*cls**sent_0*_A*mask*_movie.*sep+*', dedup=True, random_k_token=False, label_token_mode='AutoL', mapping_path='/content/drive/MyDrive/Transfer-Learning-System-for-Text-Classification-main/SRC/data/k-shot/SST-2/16-42.sort.txt', max_seq_len=128, first_sent_limit=None, other_sent_limit=None, no_finetune=False, logging_dir='outputs/logging/sst2/16-42')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "parse_args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4xerl9_6ToM"
      },
      "outputs": [],
      "source": [
        "def load_data(task_name, data_dir):\n",
        "    if task_name in [\"sst2\", \"cola\", \"mrpc\", \"qnli\", \"qqp\", \"rte\"]:\n",
        "        data_files = {\n",
        "            \"train\": os.path.join(data_dir, \"train.tsv\"),\n",
        "            \"dev\": os.path.join(data_dir, \"dev.tsv\"),\n",
        "            \"test\": os.path.join(data_dir, \"test.tsv\"),\n",
        "        }\n",
        "    elif task_name in [\"sst5\"]:\n",
        "        data_files = {\n",
        "            \"train\": os.path.join(data_dir, \"train.csv\"),\n",
        "            \"dev\": os.path.join(data_dir, \"dev.csv\"),\n",
        "            \"test\": os.path.join(data_dir, \"test.csv\"),\n",
        "        }\n",
        "\n",
        "    elif task_name in [\"mnli\"]:\n",
        "        data_files = {\n",
        "            \"train\": os.path.join(data_dir, \"train.tsv\"),\n",
        "            \"dev\": os.path.join(data_dir, \"dev_matched.tsv\"),\n",
        "            \"test_m\": os.path.join(data_dir, \"test_matched.tsv\"),\n",
        "            \"test_mm\": os.path.join(data_dir, \"test_mismatched.tsv\"),\n",
        "        }\n",
        "\n",
        "    if task_name in [\"sst2\", \"mnli\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst5\"]:\n",
        "        dataset = load_dataset('csv', data_files=data_files, delimiter='\\t', quoting=3)\n",
        "    elif task_name in [\"sst5\"]:\n",
        "        dataset = load_dataset('csv', data_files=data_files, delimiter='\\t', quoting=3)\n",
        "    elif task_name in [\"cola\"]:\n",
        "        dataset = load_dataset('csv', data_files=data_files, delimiter='\\t', column_names=[\"id\", \"label\", \"_\", \"sentence\"])\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SfNvM1V6adj"
      },
      "outputs": [],
      "source": [
        "def trim_batch(\n",
        "    input_ids,\n",
        "    pad_token_id,\n",
        "    attention_mask=None,\n",
        "):\n",
        "    \"\"\"Remove columns that are populated exclusively by pad_token_id\"\"\"\n",
        "    keep_column_mask = input_ids.ne(pad_token_id).any(dim=0)\n",
        "    if attention_mask is None:\n",
        "        return input_ids[:, keep_column_mask]\n",
        "    else:\n",
        "        return (input_ids[:, keep_column_mask], attention_mask[:, keep_column_mask])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgpl-ldh6eLT"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
        "    accelerator = Accelerator()\n",
        "    # Make one log on every process with the configuration for debugging.\n",
        "    logging_dir = os.path.join(args.output_dir, \"logging\", args.task_name, str(args.shot_num) + \"-\" + str(args.seed))\n",
        "    os.makedirs(logging_dir, exist_ok=True)\n",
        "    filename = None\n",
        "    if args.no_finetune:\n",
        "        filename = \"no_finetune\"\n",
        "    else:\n",
        "        filename = \"trainstep{}_warmupstep{}_lr{}_pbs{}\".format(args.max_train_steps, args.num_warmup_steps, args.learning_rate, args.per_device_train_batch_size)\n",
        "    filename += \"_topk{}\".format(args.top_k)\n",
        "    filename += \"_\" + args.label_token_mode\n",
        "    if args.label_token_mode == \"AMuLaP\":\n",
        "        filename += \"_random\" if args.random_k_token else \"\"\n",
        "        filename += \"_dedup\" if args.dedup else \"\"\n",
        "    filename += \".log\"\n",
        "    logging_filename = os.path.join(args.logging_dir, filename)\n",
        "    logging.basicConfig(\n",
        "        filename=logging_filename,\n",
        "        filemode=\"w\",\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    logger.info(accelerator.state)\n",
        "\n",
        "    # Setup logging, we only want one process per machine to log things on the screen.\n",
        "    # accelerator.is_local_main_process is only True for one process per machine.\n",
        "    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
        "    if accelerator.is_local_main_process:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    # If passed along, set the training seed now.\n",
        "    if args.seed is not None:\n",
        "        set_seed(args.seed)\n",
        "\n",
        "    raw_datasets = load_data(args.task_name, args.data_dir)\n",
        "\n",
        "    label2id = None\n",
        "    labels = raw_datasets[\"train\"][task_label_key[args.task_name]]\n",
        "    labels = list(set(labels))\n",
        "    label2id = {label: i for i, label in enumerate(labels)}\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    #\n",
        "    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
        "    # download model & vocab.\n",
        "    config = AutoConfig.from_pretrained(args.model_name_or_path, num_labels=len(label2id), finetuning_task=args.task_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=False)\n",
        "    model = RobertaForPromptFinetuning.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    # Preprocessing the datasets\n",
        "    def tokenize_multipart_input(\n",
        "        input_text_list,\n",
        "        max_length,\n",
        "        prompt=False,\n",
        "        template=None,\n",
        "        label_word_list=None,\n",
        "        first_sent_limit=None,\n",
        "        other_sent_limit=None,\n",
        "        gpt3=False,\n",
        "        truncate_head=False,\n",
        "        support_labels=None,\n",
        "    ):\n",
        "        def enc(text):\n",
        "            return tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "        input_ids = []\n",
        "        attention_mask = []\n",
        "        token_type_ids = [] # Only for BERT\n",
        "        mask_pos = None # Position of the mask token\n",
        "\n",
        "        if prompt:\n",
        "            \"\"\"\n",
        "            Concatenate all sentences and prompts based on the provided template.\n",
        "            Template example: '*cls*It was*mask*.*sent_0**<sep>*label_0:*sent_1**<sep>**label_1*:*sent_2**<sep>*'\n",
        "            *xx* represent variables:\n",
        "                *cls*: cls_token\n",
        "                *mask*: mask_token\n",
        "                *sep*: sep_token\n",
        "                *sep+*: sep_token, also means +1 for segment id\n",
        "                *sent_i*: sentence i (input_text_list[i])\n",
        "                *sent-_i*: same as above, but delete the last token\n",
        "                *sentl_i*: same as above, but use lower case for the first word\n",
        "                *sentl-_i*: same as above, but use lower case for the first word and delete the last token\n",
        "                *+sent_i*: same as above, but add a space before the sentence\n",
        "                *+sentl_i*: same as above, but add a space before the sentence and use lower case for the first word\n",
        "                *label_i*: label_word_list[i]\n",
        "                *label_x*: label depends on the example id (support_labels needed). this is only used in GPT-3's in-context learning\n",
        "            Use \"_\" to replace space.\n",
        "            PAY ATTENTION TO SPACE!! DO NOT leave space before variables, for this will lead to extra space token.\n",
        "            \"\"\"\n",
        "            assert template is not None\n",
        "\n",
        "            special_token_mapping = {\n",
        "                'cls': tokenizer.cls_token_id, 'mask': tokenizer.mask_token_id, 'sep': tokenizer.sep_token_id, 'sep+': tokenizer.sep_token_id,\n",
        "            }\n",
        "            template_list = template.split('*') # Get variable list in the template\n",
        "            segment_id = 0 # Current segment id. Segment id +1 if encountering sep+.\n",
        "\n",
        "            for part_id, part in enumerate(template_list):\n",
        "                new_tokens = []\n",
        "                segment_plus_1_flag = False\n",
        "                if part in special_token_mapping:\n",
        "                    if part == 'cls' and 'T5' in type(tokenizer).__name__:\n",
        "                        # T5 does not have cls token\n",
        "                        continue\n",
        "                    new_tokens.append(special_token_mapping[part])\n",
        "                    if part == 'sep+':\n",
        "                        segment_plus_1_flag = True\n",
        "                elif part[:6] == 'label_':\n",
        "                    # Note that label_word_list already has extra space, so do not add more space ahead of it.\n",
        "                    label_id = int(part.split('_')[1])\n",
        "                    label_word = label_word_list[label_id]\n",
        "                    new_tokens.append(label_word)\n",
        "                elif part[:7] == 'labelx_':\n",
        "                    instance_id = int(part.split('_')[1])\n",
        "                    label_id = support_labels[instance_id]\n",
        "                    label_word = label_word_list[label_id]\n",
        "                    new_tokens.append(label_word)\n",
        "                elif part[:5] == 'sent_':\n",
        "                    sent_id = int(part.split('_')[1])\n",
        "                    new_tokens += enc(input_text_list[sent_id])\n",
        "                elif part[:6] == '+sent_':\n",
        "                    # Add space\n",
        "                    sent_id = int(part.split('_')[1])\n",
        "                    new_tokens += enc(' ' + input_text_list[sent_id])\n",
        "                elif part[:6] == 'sent-_':\n",
        "                    # Delete the last token\n",
        "                    sent_id = int(part.split('_')[1])\n",
        "                    new_tokens += enc(input_text_list[sent_id][:-1])\n",
        "                elif part[:6] == 'sentl_':\n",
        "                    # Lower case the first token\n",
        "                    sent_id = int(part.split('_')[1])\n",
        "                    text = input_text_list[sent_id]\n",
        "                    text = text[:1].lower() + text[1:]\n",
        "                    new_tokens += enc(text)\n",
        "                elif part[:7] == '+sentl_':\n",
        "                    # Lower case the first token and add space\n",
        "                    sent_id = int(part.split('_')[1])\n",
        "                    text = input_text_list[sent_id]\n",
        "                    text = text[:1].lower() + text[1:]\n",
        "                    new_tokens += enc(' ' + text)\n",
        "                elif part[:7] == 'sentl-_':\n",
        "                    # Lower case the first token and discard the last token\n",
        "                    sent_id = int(part.split('_')[1])\n",
        "                    text = input_text_list[sent_id]\n",
        "                    text = text[:1].lower() + text[1:]\n",
        "                    new_tokens += enc(text[:-1])\n",
        "                elif part[:6] == 'sentu_':\n",
        "                    # Upper case the first token\n",
        "                    sent_id = int(part.split('_')[1])\n",
        "                    text = input_text_list[sent_id]\n",
        "                    text = text[:1].upper() + text[1:]\n",
        "                    new_tokens += enc(text)\n",
        "                elif part[:7] == '+sentu_':\n",
        "                    # Upper case the first token and add space\n",
        "                    sent_id = int(part.split('_')[1])\n",
        "                    text = input_text_list[sent_id]\n",
        "                    text = text[:1].upper() + text[1:]\n",
        "                    new_tokens += enc(' ' + text)\n",
        "                else:\n",
        "                    # Just natural language prompt\n",
        "                    part = part.replace('_', ' ')\n",
        "                    # handle special case when T5 tokenizer might add an extra space\n",
        "                    if len(part) == 1:\n",
        "                        new_tokens.append(tokenizer._convert_token_to_id(part))\n",
        "                    else:\n",
        "                        new_tokens += enc(part)\n",
        "\n",
        "                if part[:4] == 'sent' or part[1:5] == 'sent':\n",
        "                    # If this part is the sentence, limit the sentence length\n",
        "                    sent_id = int(part.split('_')[1])\n",
        "                    if sent_id == 0:\n",
        "                        if first_sent_limit is not None:\n",
        "                            new_tokens = new_tokens[:first_sent_limit]\n",
        "                    else:\n",
        "                        if other_sent_limit is not None:\n",
        "                            new_tokens = new_tokens[:other_sent_limit]\n",
        "\n",
        "                input_ids += new_tokens\n",
        "                attention_mask += [1 for i in range(len(new_tokens))]\n",
        "                token_type_ids += [segment_id for i in range(len(new_tokens))]\n",
        "\n",
        "                if segment_plus_1_flag:\n",
        "                    segment_id += 1\n",
        "        else:\n",
        "            input_ids = [tokenizer.cls_token_id]\n",
        "            attention_mask = [1]\n",
        "            token_type_ids = [0]\n",
        "\n",
        "            for sent_id, input_text in enumerate(input_text_list):\n",
        "                if input_text is None:\n",
        "                    # Do not have text_b\n",
        "                    continue\n",
        "                if pd.isna(input_text) or input_text is None:\n",
        "                    # Empty input\n",
        "                    input_text = ''\n",
        "                input_tokens = enc(input_text) + [tokenizer.sep_token_id]\n",
        "                input_ids += input_tokens\n",
        "                attention_mask += [1 for i in range(len(input_tokens))]\n",
        "                token_type_ids += [sent_id for i in range(len(input_tokens))]\n",
        "\n",
        "            if 'T5' in type(tokenizer).__name__: # T5 does not have CLS token\n",
        "                input_ids = input_ids[1:]\n",
        "                attention_mask = attention_mask[1:]\n",
        "                token_type_ids = token_type_ids[1:]\n",
        "\n",
        "        # Padding\n",
        "        if first_sent_limit is not None and len(input_ids) > max_length:\n",
        "            # If using sentence limit, the total length still exceeds the maximum limit, report a warning\n",
        "            logger.warn(\"Input exceeds max_length limit: {}\".format(tokenizer.decode(input_ids)))\n",
        "\n",
        "        while len(input_ids) < max_length:\n",
        "            input_ids.append(tokenizer.pad_token_id)\n",
        "            attention_mask.append(0)\n",
        "            token_type_ids.append(0)\n",
        "\n",
        "        # Truncate\n",
        "        if len(input_ids) > max_length:\n",
        "            if truncate_head:\n",
        "                input_ids = input_ids[-max_length:]\n",
        "                attention_mask = attention_mask[-max_length:]\n",
        "                token_type_ids = token_type_ids[-max_length:]\n",
        "            else:\n",
        "                # Default is to truncate the tail\n",
        "                input_ids = input_ids[:max_length]\n",
        "                attention_mask = attention_mask[:max_length]\n",
        "                token_type_ids = token_type_ids[:max_length]\n",
        "\n",
        "        # Find mask token\n",
        "        if prompt:\n",
        "            mask_pos = [input_ids.index(tokenizer.mask_token_id)]\n",
        "            # Make sure that the masked position is inside the max_length\n",
        "            assert mask_pos[0] < max_length\n",
        "\n",
        "        result = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
        "        if 'BERT' in type(tokenizer).__name__:\n",
        "            # Only provide token type ids for BERT\n",
        "            result['token_type_ids'] = token_type_ids\n",
        "\n",
        "        if prompt:\n",
        "            result['mask_pos'] = mask_pos\n",
        "\n",
        "        return result\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        # Tokenize the texts\n",
        "        result = {}\n",
        "        result[\"input_ids\"] = []\n",
        "        result[\"attention_mask\"] = []\n",
        "        result[\"mask_pos\"] = []\n",
        "\n",
        "        if len(task_input_key[args.task_name]) == 1:\n",
        "            sentences = examples[task_input_key[args.task_name][0]]\n",
        "            input_text_lists = [[sent] for sent in sentences]\n",
        "        else:\n",
        "            sentences1 = examples[task_input_key[args.task_name][0]]\n",
        "            sentences2 = examples[task_input_key[args.task_name][1]]\n",
        "            input_text_lists = [[sent1, sent2] for sent1, sent2 in zip(sentences1, sentences2)]\n",
        "\n",
        "        for input_text_list in input_text_lists:\n",
        "            res = tokenize_multipart_input(\n",
        "                input_text_list=input_text_list,\n",
        "                max_length=args.max_seq_len,\n",
        "                prompt=True,\n",
        "                template=args.template,\n",
        "                first_sent_limit=args.first_sent_limit,\n",
        "                other_sent_limit=args.other_sent_limit,\n",
        "                )\n",
        "\n",
        "            result[\"input_ids\"].append(res[\"input_ids\"])\n",
        "            result[\"attention_mask\"].append(res[\"attention_mask\"])\n",
        "            result[\"mask_pos\"].append(res[\"mask_pos\"])\n",
        "\n",
        "        if task_label_key[args.task_name] in examples:\n",
        "            result[\"labels\"] = [label2id[label] for label in examples[task_label_key[args.task_name]]]\n",
        "\n",
        "        return result\n",
        "\n",
        "    processed_datasets = raw_datasets.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=raw_datasets[\"train\"].column_names,\n",
        "    )\n",
        "\n",
        "    train_dataset = processed_datasets[\"train\"]\n",
        "    eval_dataset = processed_datasets[\"dev\"]\n",
        "\n",
        "    # Log a few random samples from the training set:\n",
        "    for index in random.sample(range(len(train_dataset)), 1):\n",
        "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
        "\n",
        "    # DataLoaders creation:\n",
        "    data_collator = default_data_collator\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
        "    )\n",
        "    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n",
        "\n",
        "    # Optimizer\n",
        "    # Split weights in two groups, one with weight decay and the other not.\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
        "\n",
        "    # Prepare everything with our `accelerator`.\n",
        "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, eval_dataloader\n",
        "    )\n",
        "\n",
        "    # Note -> the training dataloader needs to be prepared before we grab his length below (cause its length will be\n",
        "    # shorter in multiprocess)\n",
        "\n",
        "    # get top-k token index\n",
        "    k_map = {}\n",
        "    if args.label_token_mode == \"AMuLaP\":\n",
        "        model.eval()\n",
        "        all_train_logits = {}\n",
        "        with torch.no_grad():\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"mask_pos\"])\n",
        "                for i in range(len(batch[\"labels\"])):\n",
        "                    if batch[\"labels\"][i].item() not in all_train_logits:\n",
        "                        all_train_logits[batch[\"labels\"][i].item()] = outputs[0][i].cpu()\n",
        "                    else:\n",
        "                        all_train_logits[batch[\"labels\"][i].item()] += outputs[0][i].cpu()\n",
        "\n",
        "        map_index = {}\n",
        "        for key in label2id:\n",
        "            label = label2id[key]\n",
        "            all_train_logits[label] = all_train_logits[label] / args.shot_num\n",
        "            sorted_logits, sort_index = torch.sort(all_train_logits[label], descending=True)\n",
        "            map_index[label] = sort_index.tolist()\n",
        "\n",
        "        label_token_set = {}\n",
        "        for key in label2id:\n",
        "            label_token_set[label2id[key]] = []\n",
        "\n",
        "        for i in range(tokenizer.vocab_size):\n",
        "            logits = [all_train_logits[label2id[key]][i] for key in label2id]\n",
        "            label_token_set[logits.index(max(logits))].append({\n",
        "                \"idx\": i,\n",
        "                \"prob\": max(logits),\n",
        "            })\n",
        "        print( label_token_set)\n",
        "\n",
        "        def myFunc(e):\n",
        "            return e['prob']\n",
        "\n",
        "        for key in label2id:\n",
        "            label = label2id[key]\n",
        "            label_token_set[label].sort(reverse=True, key=myFunc)\n",
        "        print(label_token_set)\n",
        "        print(tokenizer.decode(label_token_set))\n",
        "\n",
        "\n",
        "        if args.dedup:\n",
        "            for key in label2id:\n",
        "                label = label2id[key]\n",
        "                k_map[label] = []\n",
        "                for i in range(args.top_k):\n",
        "                    k_map[label].append(label_token_set[label][i][\"idx\"])\n",
        "        elif args.random_k_token:\n",
        "            num_list = random.sample(range(tokenizer.vocab_size), args.top_k * len(label2id))\n",
        "            for i, key in enumerate(label2id):\n",
        "                label = label2id[key]\n",
        "                k_map[label] = num_list[i * args.top_k : (i+1) * args.top_k]\n",
        "        else:\n",
        "            for key in label2id:\n",
        "                label = label2id[key]\n",
        "                k_map[label] = map_index[label][:args.top_k]\n",
        "        print(k_map[label])\n",
        "        print(tokenizer.decode(k_map[label]))\n",
        "        for label in k_map:\n",
        "          print(f\"Top-{args.top_k} token indices for label {label}:\")\n",
        "          print(k_map[label])\n",
        "          print(tokenizer.decode(k_map[label]))\n",
        "\n",
        "\n",
        "    elif args.label_token_mode == \"AutoL\":\n",
        "        label_to_word = {}\n",
        "        for key in label2id:\n",
        "            label = label2id[key]\n",
        "            label_to_word[label] = []\n",
        "        # seed_mapping_path = os.path.join(args.mapping_path, \"{}-{}.sort.txt\".format(args.shot_num, args.seed))\n",
        "        with open(args.mapping_path) as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                line = eval(line)\n",
        "                for key in line:\n",
        "                    word = line[key]\n",
        "                    if word[0] not in ['<', '[', '.', ',']:\n",
        "                        assert len(tokenizer.tokenize(' ' + word)) == 1\n",
        "                        word = tokenizer._convert_token_to_id(tokenizer.tokenize(' ' + word)[0])\n",
        "                    else:\n",
        "                        word = tokenizer._convert_token_to_id(word)\n",
        "                    if len(key) == 1:\n",
        "                        label_to_word[label2id[int(key)]].append(word)\n",
        "                    else:\n",
        "                        label_to_word[label2id[key]].append(word)\n",
        "        for key in label2id:\n",
        "            label = label2id[key]\n",
        "            k_map[label] = label_to_word[label][:args.top_k]\n",
        "    elif args.label_token_mode == \"PETAL\":\n",
        "        label_to_word = {}\n",
        "        for key in label2id:\n",
        "            label = label2id[key]\n",
        "            label_to_word[label] = []\n",
        "        # seed_mapping_path = os.path.join(args.mapping_path, \"{}-{}.json\".format(args.shot_num, args.seed))\n",
        "        with open(args.mapping_path) as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                line = json.loads(line)\n",
        "                for key in line:\n",
        "                    for word in line[key]:\n",
        "                        if word[0] not in ['<', '[', '.', ',']:\n",
        "                            if len(tokenizer.tokenize(' ' + word)) == 1:\n",
        "                                word = tokenizer._convert_token_to_id(tokenizer.tokenize(' ' + word)[0])\n",
        "                            else:\n",
        "                                word = tokenizer._convert_token_to_id(word)\n",
        "                        else:\n",
        "                            word = tokenizer._convert_token_to_id(word)\n",
        "\n",
        "                        if len(key) == 1:\n",
        "                            label_to_word[label2id[int(key)]].append(word)\n",
        "                        else:\n",
        "                            label_to_word[label2id[key]].append(word)\n",
        "        for key in label2id:\n",
        "            label = label2id[key]\n",
        "            k_map[label] = label_to_word[label][:args.top_k]\n",
        "\n",
        "    model.label_token_list = {}\n",
        "    for key in label2id:\n",
        "        label = label2id[key]\n",
        "        model.label_token_list[label] = torch.tensor(k_map[label]).long().cuda()\n",
        "\n",
        "    # Scheduler and math around the number of training steps.\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
        "    if args.max_train_steps is None:\n",
        "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
        "    else:\n",
        "        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "    lr_scheduler = get_scheduler(\n",
        "        name=args.lr_scheduler_type,\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=args.num_warmup_steps,\n",
        "        num_training_steps=args.max_train_steps,\n",
        "    )\n",
        "\n",
        "    # Get the metric function\n",
        "    metric = load_metric(\"glue\", args.task_name)\n",
        "\n",
        "    if args.no_finetune:\n",
        "        for split in [\"dev\", \"test\"] if args.task_name != \"mnli\" else [\"dev\", \"test_m\", \"test_mm\"]:\n",
        "            eval_dataset = processed_datasets[split]\n",
        "            eval_dataloader = DataLoader(\n",
        "                eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n",
        "            )\n",
        "            eval_dataloader = accelerator.prepare(eval_dataloader)\n",
        "\n",
        "            model.eval()\n",
        "            for step, batch in enumerate(eval_dataloader):\n",
        "                outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"mask_pos\"])\n",
        "                predictions = outputs[0].argmax(dim=-1)\n",
        "                metric.add_batch(\n",
        "                    predictions=accelerator.gather(predictions),\n",
        "                    references=accelerator.gather(batch[\"labels\"]),\n",
        "                )\n",
        "\n",
        "            eval_metric = metric.compute()\n",
        "            logger.info(f\"{split}: {eval_metric}\")\n",
        "        return\n",
        "\n",
        "    # Train!\n",
        "    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
        "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
        "    # Only show the progress bar once on each machine.\n",
        "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "    completed_steps = 0\n",
        "\n",
        "    best_metric = -1\n",
        "    best_metric_step = None\n",
        "\n",
        "    for epoch in range(args.num_train_epochs):\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"mask_pos\"], batch[\"labels\"])\n",
        "            loss = outputs[0]\n",
        "            loss = loss / args.gradient_accumulation_steps\n",
        "            accelerator.backward(loss)\n",
        "            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                progress_bar.update(1)\n",
        "                completed_steps += 1\n",
        "\n",
        "            if completed_steps % args.logging_loss_steps == 0 and completed_steps != 0:\n",
        "                logger.info(f\"step {completed_steps}: loss: {loss}\")\n",
        "\n",
        "            if completed_steps >= args.max_train_steps:\n",
        "                break\n",
        "\n",
        "            if args.eval_steps is not None:\n",
        "                if completed_steps % args.eval_steps == 0 and completed_steps != 0:\n",
        "                    model.eval()\n",
        "                    for eval_step, eval_batch in enumerate(eval_dataloader):\n",
        "                        outputs = model(eval_batch[\"input_ids\"], eval_batch[\"attention_mask\"], eval_batch[\"mask_pos\"])\n",
        "                        predictions = outputs[0].argmax(dim=-1)\n",
        "                        metric.add_batch(\n",
        "                            predictions=accelerator.gather(predictions),\n",
        "                            references=accelerator.gather(eval_batch[\"labels\"]),\n",
        "                        )\n",
        "\n",
        "                    eval_metric = metric.compute()\n",
        "                    logger.info(f\"step {completed_steps}: {eval_metric}\")\n",
        "                    if eval_metric[task_metric[args.task_name]] > best_metric:\n",
        "                        best_metric = eval_metric[task_metric[args.task_name]]\n",
        "                        best_metric_step = completed_steps\n",
        "                        accelerator.wait_for_everyone()\n",
        "                        unwrapped_model = accelerator.unwrap_model(model)\n",
        "                        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n",
        "                    model.train()\n",
        "\n",
        "    model.eval()\n",
        "    for eval_step, eval_batch in enumerate(eval_dataloader):\n",
        "        outputs = model(eval_batch[\"input_ids\"], eval_batch[\"attention_mask\"], eval_batch[\"mask_pos\"])\n",
        "        predictions = outputs[0].argmax(dim=-1)\n",
        "        metric.add_batch(\n",
        "            predictions=accelerator.gather(predictions),\n",
        "            references=accelerator.gather(eval_batch[\"labels\"]),\n",
        "        )\n",
        "\n",
        "    eval_metric = metric.compute()\n",
        "    logger.info(f\"step {completed_steps}: {eval_metric}\")\n",
        "    if eval_metric[task_metric[args.task_name]] > best_metric:\n",
        "        best_metric = eval_metric[task_metric[args.task_name]]\n",
        "        best_metric_step = completed_steps\n",
        "        accelerator.wait_for_everyone()\n",
        "        unwrapped_model = accelerator.unwrap_model(model)\n",
        "        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n",
        "\n",
        "    logger.info(f\"early stop at step {best_metric_step}, metric: {best_metric}\")\n",
        "\n",
        "    model = RobertaForPromptFinetuning.from_pretrained(\n",
        "        args.output_dir,\n",
        "        from_tf=bool(\".ckpt\" in args.output_dir),\n",
        "        config=config,\n",
        "        )\n",
        "\n",
        "    model.label_token_list = {}\n",
        "    for key in label2id:\n",
        "        label = label2id[key]\n",
        "        model.label_token_list[label] = torch.tensor(k_map[label]).long().cuda()\n",
        "    model = accelerator.prepare(model)\n",
        "\n",
        "    for split in [\"test\"] if args.task_name != \"mnli\" else [\"test_m\", \"test_mm\"]:\n",
        "        eval_dataset = processed_datasets[split]\n",
        "        eval_dataloader = DataLoader(\n",
        "            eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n",
        "        )\n",
        "        eval_dataloader = accelerator.prepare(eval_dataloader)\n",
        "\n",
        "        model.eval()\n",
        "        for step, batch in enumerate(eval_dataloader):\n",
        "            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"mask_pos\"])\n",
        "            predictions = outputs[0].argmax(dim=-1)\n",
        "            metric.add_batch(\n",
        "                predictions=accelerator.gather(predictions),\n",
        "                references=accelerator.gather(batch[\"labels\"]),\n",
        "            )\n",
        "\n",
        "        eval_metric = metric.compute()\n",
        "        logger.info(f\"{split}: {eval_metric}\")\n",
        "\n",
        "    # Delete the saved outputs to save space\n",
        "    shutil.rmtree(args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "10a10ae9541d4712886ef239065d7413",
            "02e77c58b60c49d2af932a94850c1189",
            "a00d9f93026f4afbb2de728112321c56",
            "9af2efae63cc4e638c0d3619bd87c2c4",
            "1bfa139057964c08ac9923a32d7d12ea",
            "47c71ebaab85476a9691af7ef110eab4",
            "bdbe845226264e77863ae5cbd238cc61",
            "7c7c07b9b44b4d4aa9b1450c28cb2ec7",
            "c48657b9509145be94f48e6c4cd00dbe",
            "a64895eb7d8e43dc82525a4bc05ffb80",
            "03a8189a68b0438cae84f0d07e8fdfb8",
            "81c517348ab84c128b683ab5884c6717",
            "d8d132ef142842f9828fce8881fbb2e7",
            "abfa3fc5bb6d4ff88e1cdda6a4112ff4",
            "5a146f3c1e534125bca07b19f5b0bf79",
            "07d287fec346423d8cfae66521c91f9d",
            "f615ef36aea144a1b250baa038957f11",
            "e7ed8f5f6c894e2bbd54f85c8883c4a5",
            "b95f3ac75e1448c4ad3ee9811c15c9c4",
            "16e91f80a2bd4f679adb141eb2eb88c3",
            "133a08b4726348cbaa59d233eee50dec",
            "ba55701d95da471d8e750dd923a029f2",
            "2e5ecd9d039c4c18adfb93ce13b8d656",
            "ffa110d7b36b4b69874732fcc2702342",
            "7108f78fad9d4541871e086b502561bd",
            "601847956c974e24af0b3258c91db416",
            "f16c457bd8304d2ba4ea1c5c343cb4db",
            "2ff7e1d95f674f04a2b84e44ee0989a5",
            "97f7cb53b45c452180969c75881a8eac",
            "9121239fb5b94d879f611398a5e91ce6",
            "fd2b6707671d4ae3ba3cbbb93bfb1501",
            "0d2a8b8ace4e40448ca9d26b86213b7a",
            "7361e22849cd4066a68d4477f70abb83"
          ]
        },
        "id": "O2945hQRKYug",
        "outputId": "c08a167c-1f14-48f7-cc45-356b7c526c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"sst2\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/merges.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/tokenizer.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/model.safetensors\n",
            "All model checkpoint weights were used when initializing RobertaForPromptFinetuning.\n",
            "\n",
            "Some weights of RobertaForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10a10ae9541d4712886ef239065d7413"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81c517348ab84c128b683ab5884c6717"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e5ecd9d039c4c18adfb93ce13b8d656"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Sample 7 of the training set: {'input_ids': [0, 9996, 1115, 162, 2156, 939, 794, 42, 1569, 479, 83, 50264, 1569, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mask_pos': [11], 'labels': 0}.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \u001b[1;32m\"<ipython-input-40-972361fa1b80>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35m<cell line: 1>\u001b[0m\n    main()\n",
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-8280d19fd5ef>\"\u001b[0;36m, line \u001b[0;32m411\u001b[0;36m, in \u001b[0;35mmain\u001b[0;36m\u001b[0m\n\u001b[0;31m    line = eval(line)\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    *cls**sent_0*_A*mask*_one.*sep+*\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10a10ae9541d4712886ef239065d7413": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02e77c58b60c49d2af932a94850c1189",
              "IPY_MODEL_a00d9f93026f4afbb2de728112321c56",
              "IPY_MODEL_9af2efae63cc4e638c0d3619bd87c2c4"
            ],
            "layout": "IPY_MODEL_1bfa139057964c08ac9923a32d7d12ea"
          }
        },
        "02e77c58b60c49d2af932a94850c1189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47c71ebaab85476a9691af7ef110eab4",
            "placeholder": "​",
            "style": "IPY_MODEL_bdbe845226264e77863ae5cbd238cc61",
            "value": "Map: 100%"
          }
        },
        "a00d9f93026f4afbb2de728112321c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c7c07b9b44b4d4aa9b1450c28cb2ec7",
            "max": 32,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c48657b9509145be94f48e6c4cd00dbe",
            "value": 32
          }
        },
        "9af2efae63cc4e638c0d3619bd87c2c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a64895eb7d8e43dc82525a4bc05ffb80",
            "placeholder": "​",
            "style": "IPY_MODEL_03a8189a68b0438cae84f0d07e8fdfb8",
            "value": " 32/32 [00:00&lt;00:00, 212.69 examples/s]"
          }
        },
        "1bfa139057964c08ac9923a32d7d12ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47c71ebaab85476a9691af7ef110eab4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdbe845226264e77863ae5cbd238cc61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c7c07b9b44b4d4aa9b1450c28cb2ec7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c48657b9509145be94f48e6c4cd00dbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a64895eb7d8e43dc82525a4bc05ffb80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03a8189a68b0438cae84f0d07e8fdfb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81c517348ab84c128b683ab5884c6717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8d132ef142842f9828fce8881fbb2e7",
              "IPY_MODEL_abfa3fc5bb6d4ff88e1cdda6a4112ff4",
              "IPY_MODEL_5a146f3c1e534125bca07b19f5b0bf79"
            ],
            "layout": "IPY_MODEL_07d287fec346423d8cfae66521c91f9d"
          }
        },
        "d8d132ef142842f9828fce8881fbb2e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f615ef36aea144a1b250baa038957f11",
            "placeholder": "​",
            "style": "IPY_MODEL_e7ed8f5f6c894e2bbd54f85c8883c4a5",
            "value": "Map: 100%"
          }
        },
        "abfa3fc5bb6d4ff88e1cdda6a4112ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b95f3ac75e1448c4ad3ee9811c15c9c4",
            "max": 32,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16e91f80a2bd4f679adb141eb2eb88c3",
            "value": 32
          }
        },
        "5a146f3c1e534125bca07b19f5b0bf79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_133a08b4726348cbaa59d233eee50dec",
            "placeholder": "​",
            "style": "IPY_MODEL_ba55701d95da471d8e750dd923a029f2",
            "value": " 32/32 [00:00&lt;00:00, 252.97 examples/s]"
          }
        },
        "07d287fec346423d8cfae66521c91f9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f615ef36aea144a1b250baa038957f11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7ed8f5f6c894e2bbd54f85c8883c4a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b95f3ac75e1448c4ad3ee9811c15c9c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16e91f80a2bd4f679adb141eb2eb88c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "133a08b4726348cbaa59d233eee50dec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba55701d95da471d8e750dd923a029f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e5ecd9d039c4c18adfb93ce13b8d656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ffa110d7b36b4b69874732fcc2702342",
              "IPY_MODEL_7108f78fad9d4541871e086b502561bd",
              "IPY_MODEL_601847956c974e24af0b3258c91db416"
            ],
            "layout": "IPY_MODEL_f16c457bd8304d2ba4ea1c5c343cb4db"
          }
        },
        "ffa110d7b36b4b69874732fcc2702342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ff7e1d95f674f04a2b84e44ee0989a5",
            "placeholder": "​",
            "style": "IPY_MODEL_97f7cb53b45c452180969c75881a8eac",
            "value": "Map: 100%"
          }
        },
        "7108f78fad9d4541871e086b502561bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9121239fb5b94d879f611398a5e91ce6",
            "max": 872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd2b6707671d4ae3ba3cbbb93bfb1501",
            "value": 872
          }
        },
        "601847956c974e24af0b3258c91db416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d2a8b8ace4e40448ca9d26b86213b7a",
            "placeholder": "​",
            "style": "IPY_MODEL_7361e22849cd4066a68d4477f70abb83",
            "value": " 872/872 [00:02&lt;00:00, 356.52 examples/s]"
          }
        },
        "f16c457bd8304d2ba4ea1c5c343cb4db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ff7e1d95f674f04a2b84e44ee0989a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97f7cb53b45c452180969c75881a8eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9121239fb5b94d879f611398a5e91ce6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd2b6707671d4ae3ba3cbbb93bfb1501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d2a8b8ace4e40448ca9d26b86213b7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7361e22849cd4066a68d4477f70abb83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}